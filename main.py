import requests
import asyncio
from urllib.parse import urlparse
import os
from dotenv import load_dotenv
import json
import logging

# âœ… ä¼˜åŒ–ï¼šè®¾ç½®æ—¥å¿—çº§åˆ«ï¼Œå‡å°‘è¾“å‡º
logging.basicConfig(level=logging.WARNING)

from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy
from crawl4ai.deep_crawling import BestFirstCrawlingStrategy
from crawl4ai.deep_crawling.filters import FilterChain, DomainFilter, URLPatternFilter, ContentTypeFilter
from crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer
from openai import OpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv('config.env')

# ============ 1. Google æœç´¢ ============

def google_search(query: str, api_key: str, cse_id: str, num_results: int = 30):
    """è°ƒç”¨ Google Custom Search APIï¼Œè¿”å› URL åˆ—è¡¨"""
    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "q": query,
        "key": api_key,
        "cx": cse_id,
        "num": 10
    }
    results = []
    start = 1
    while len(results) < num_results:
        params["start"] = start
        resp = requests.get(url, params=params).json()
        items = resp.get("items", [])
        for item in items:
            results.append(item["link"])
        start += 10
        if not items:
            break
    return results[:num_results]


def get_domain(url: str):
    return urlparse(url).netloc


# ============ 2. æ·±åº¦çˆ¬å–å®˜ç½‘ ============

async def deep_crawl(url: str):
    """ç”¨ crawl4ai å¯¹ä¸€ä¸ªç½‘å€åšæ·±åº¦çˆ¬å–"""
    filter_chain = FilterChain([
        DomainFilter(allowed_domains=[get_domain(url)]),
        URLPatternFilter(patterns=["*contact*", "*about*", "*team*", "*procurement*"]),
        ContentTypeFilter(allowed_types=["text/html"])
    ])

    keyword_scorer = KeywordRelevanceScorer(
        keywords=["purchase", "procurement", "buyer", "contact", "email"],
        weight=0.8
    )

    config = CrawlerRunConfig(
        deep_crawl_strategy=BestFirstCrawlingStrategy(
            max_depth=2,
            include_external=False,
            filter_chain=filter_chain,
            url_scorer=keyword_scorer
        ),
        scraping_strategy=LXMLWebScrapingStrategy(),
        stream=True,
        verbose=False
    )

    async with AsyncWebCrawler() as crawler:
        pages = []
        async for result in await crawler.arun(url, config=config):
            pages.append(result)
        return pages


# ============ 3. AI æ¸…æ´—è”ç³»äºº ============

async def clean_contacts_with_openai(urls, openai_api_key: str):
    """ä½¿ç”¨ OpenAI API å¯¹é¡µé¢å†…å®¹åšAIæ¸…æ´—ï¼Œæå–è”ç³»äººä¿¡æ¯"""
    client = OpenAI(api_key=openai_api_key)
    
    results = []
    async with AsyncWebCrawler() as crawler:
        for url in urls:
            try:
                # è·å–é¡µé¢å†…å®¹
                result = await crawler.arun(url)
                if result.success and result.markdown:
                    content = result.markdown
                    
                    # ä½¿ç”¨ OpenAI æå–è”ç³»äººä¿¡æ¯
                    response = client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[
                            {
                                "role": "system",
                                "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è”ç³»äººä¿¡æ¯æå–åŠ©æ‰‹ã€‚è¯·ä»ç½‘é¡µå†…å®¹ä¸­æå–è”ç³»äººä¿¡æ¯ï¼Œç‰¹åˆ«å…³æ³¨é‡‡è´­ã€é‡‡è´­ç»ç†ã€é‡‡è´­è´Ÿè´£äººç­‰ç›¸å…³èŒä½ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºç»“æœï¼š
{
  "contacts_found": [
    {
      "name": "è”ç³»äººå§“å",
      "position": "èŒä½",
      "email": "é‚®ç®±åœ°å€",
      "phone": "ç”µè¯å·ç ",
      "department": "éƒ¨é—¨",
      "is_procurement_related": true/false
    }
  ],
  "search_hints": ["å¦‚æœæ²¡æ‰¾åˆ°è”ç³»äººï¼Œæä¾›åç»­æœç´¢å»ºè®®"]
}

å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•è”ç³»äººä¿¡æ¯ï¼Œè¯·å°† contacts_found è®¾ä¸ºç©ºæ•°ç»„ï¼Œå¹¶åœ¨ search_hints ä¸­æä¾›æœç´¢å»ºè®®ã€‚"""
                            },
                            {
                                "role": "user",
                                "content": f"è¯·ä»ä»¥ä¸‹ç½‘é¡µå†…å®¹ä¸­æå–è”ç³»äººä¿¡æ¯ï¼š\n\nURL: {url}\n\nå†…å®¹ï¼š\n{content[:4000]}"  # é™åˆ¶å†…å®¹é•¿åº¦
                            }
                        ],
                        temperature=0.1
                    )
                    
                    try:
                        extracted_data = json.loads(response.choices[0].message.content)
                        results.append({
                            "url": url,
                            "contacts": extracted_data.get("contacts_found", []),
                            "search_hints": extracted_data.get("search_hints", [])
                        })
                    except json.JSONDecodeError:
                        # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•æå–æ–‡æœ¬ä¸­çš„ä¿¡æ¯
                        text_content = response.choices[0].message.content
                        results.append({
                            "url": url,
                            "contacts": [],
                            "search_hints": [text_content],
                            "raw_response": text_content
                        })
                        
            except Exception as e:
                print(f"Error processing {url}: {e}")
                results.append({
                    "url": url,
                    "contacts": [],
                    "search_hints": [f"å¤„ç†é”™è¯¯: {str(e)}"],
                    "error": str(e)
                })
    
    return results


# ============ 4. æ€»ä½“æµç¨‹ ============

def investigate_company(company_name, google_api_key, cse_id, openai_api_key):
    """è°ƒæŸ¥å…¬å¸å¹¶æå–é‡‡è´­è”ç³»äººä¿¡æ¯"""
    print(f"ğŸ” æ­£åœ¨æœç´¢å…¬å¸: {company_name}")
    
    # Step 1: Google æœç´¢
    urls = google_search(company_name, google_api_key, cse_id)
    print(f"æ‰¾åˆ° {len(urls)} ä¸ªå€™é€‰ç½‘å€")
    
    if not urls:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç½‘å€")
        return []
    
    # Step 2: æ·±åº¦çˆ¬å–
    print("ğŸŒ æ­£åœ¨çˆ¬å–å€™é€‰ç½‘å€...")
    all_urls = []
    for url in urls[:3]:  # é™åˆ¶å‰3ä¸ªURLé¿å…è¿‡å¤šè¯·æ±‚
        try:
            pages = asyncio.run(deep_crawl(url))
            all_urls.extend([p.url for p in pages])
            print(f"ä» {url} çˆ¬å–äº† {len(pages)} ä¸ªé¡µé¢")
        except Exception as e:
            print(f"çˆ¬å– {url} æ—¶å‡ºé”™: {e}")
    
    if not all_urls:
        print("âŒ æ²¡æœ‰æˆåŠŸçˆ¬å–åˆ°ä»»ä½•é¡µé¢")
        return []
    
    print(f"æ€»å…±æ”¶é›†åˆ° {len(all_urls)} ä¸ªé¡µé¢URL")
    
    # Step 3: AI æ¸…æ´—è”ç³»äºº
    print("ğŸ¤– æ­£åœ¨ç”¨ AI æå–è”ç³»äººä¿¡æ¯...")
    contacts_results = asyncio.run(clean_contacts_with_openai(all_urls, openai_api_key))
    
    # æ±‡æ€»ç»“æœ
    all_contacts = []
    all_search_hints = []
    
    for result in contacts_results:
        all_contacts.extend(result["contacts"])
        all_search_hints.extend(result["search_hints"])
    
    # è¿‡æ»¤å‡ºé‡‡è´­ç›¸å…³è”ç³»äºº
    procurement_contacts = [c for c in all_contacts if c.get("is_procurement_related", False)]
    
    print(f"\nğŸ“Š ç»“æœç»Ÿè®¡:")
    print(f"æ€»è”ç³»äºº: {len(all_contacts)}")
    print(f"é‡‡è´­ç›¸å…³è”ç³»äºº: {len(procurement_contacts)}")
    
    if procurement_contacts:
        print("\nâœ… æ‰¾åˆ°é‡‡è´­ç›¸å…³è”ç³»äººï¼š")
        for contact in procurement_contacts:
            print(f"  - {contact.get('name', 'N/A')} ({contact.get('position', 'N/A')})")
            if contact.get('email'):
                print(f"    é‚®ç®±: {contact['email']}")
            if contact.get('phone'):
                print(f"    ç”µè¯: {contact['phone']}")
            print()
    elif all_contacts:
        print("\nâš ï¸ æ‰¾åˆ°è”ç³»äººä½†æ— é‡‡è´­ç›¸å…³ï¼š")
        for contact in all_contacts[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"  - {contact.get('name', 'N/A')} ({contact.get('position', 'N/A')})")
    else:
        print("\nâŒ æœªæ‰¾åˆ°ä»»ä½•è”ç³»äººä¿¡æ¯")
        if all_search_hints:
            print("æœç´¢å»ºè®®ï¼š")
            for hint in all_search_hints[:3]:
                print(f"  - {hint}")
    
    return {
        "company": company_name,
        "total_contacts": len(all_contacts),
        "procurement_contacts": len(procurement_contacts),
        "contacts": all_contacts,
        "procurement_contacts_list": procurement_contacts,
        "search_hints": all_search_hints
    }


# ============ 5. ä¸»ç¨‹åºå…¥å£ ============

if __name__ == "__main__":
    # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
    GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
    CSE_ID = os.getenv('CSE_ID')
    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
    
    if not all([GOOGLE_API_KEY, CSE_ID, OPENAI_API_KEY]):
        print("âŒ è¯·ç¡®ä¿åœ¨ config.env æ–‡ä»¶ä¸­é…ç½®äº†æ‰€æœ‰å¿…éœ€çš„APIå¯†é’¥ï¼š")
        print("   - GOOGLE_API_KEY")
        print("   - CSE_ID")
        print("   - OPENAI_API_KEY")
        exit(1)
    
    # æµ‹è¯•å…¬å¸
    company = "Shree Prayag Air Controls"
    
    print("ğŸš€ é‡‡è´­è”ç³»äººæŸ¥æ‰¾å™¨å¯åŠ¨")
    print("=" * 50)
    
    result = investigate_company(company, GOOGLE_API_KEY, CSE_ID, OPENAI_API_KEY)
    
    print("\n" + "=" * 50)
    print("ğŸ¯ è°ƒæŸ¥å®Œæˆ")
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    if result:
        with open(f"{company.replace(' ', '_')}_contacts.json", "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {company.replace(' ', '_')}_contacts.json")
